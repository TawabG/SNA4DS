---
title: "Netflix case study: Genre vs decade"
author:
- name: Alan, Chantal, Dion, Dylan, Suzanne, Tawab
  role: SNA4DS Group 14 students
shorttitle: Research Project Report
output:
  html_document:
    df_print: paged
authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
  Enter author note here.
keywords: keywords
wordcount: X
bibliography: r-references.bib
toc: yes
toc_float: yes
floatsintext: yes
figurelist: no
tablelist: no
footnotelist: no
linenumbers: no
mask: no
draft: no
documentclass: apa6
classoption: man
affiliation:
- id: '1'
  institution: Eindhoven University of Technology
- id: '2'
  institution: Tilburg University
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
source("same_size_ggplot.R")
library(ggplot2)
library(dplyr)
library(kableExtra)
library(scales)
library(gridExtra)
library(lubridate)
library(nycflights13)
library(tidyquant)
library(readxl)
```

```{r analysis-preferences, include = FALSE}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```



# Citations in papaja, detele appropriately later

Add the bibtex entry in the .bib file. You can find the entries in Google scholar, 
but double check since it is not always correct. 

Call the citations in the text:

Citation within parentheses	[@R-papaja]

Multiple citations	[@R-papaja; @R-base]

In-text citations	@R-papaja

Year only	[-@R-base]

Only if your citation appears in the text it will also show up in the Reference 
list. Don't manually modify the Reference list.

## Executive Summary 

(150 words) – 0.3 POINTS
Summarize the report. Write this as the very last thing.

What is the main topic you are addressing?

what are your research questions and hypotheses?  

what are your results and the main conclusion?



## Introduction 
### Main topic
In this paper, we are going to study the presence of social networks within a movie streaming platform. We're focusing on the structure of links among a group of social players, which consist of users watching and rating movies on Netflix.  
Users of Netflix's movie recommendation algorithms are frequently given specific questions about their interests for certain items (which they provide by liking or disliking them, for example). These choices are then immediately integrated into the underlying learning system for future suggestions. If a recommender system starts promoting unwanted products after incorporating new preferences, the user may try to steer the system in the future by correcting it or supplying alternate preference information. 

### Importance
It is important to study the presence of these socials networks because this could potentially improve the recommender engine that this currently in place. For example, if you know that a user is likely to like a movie that other users with the same “liking profile” also like, you can recommend that movie to the user. When these connections are studied thoroughly, you could have a high probability that the recommendation is successful. This could have a large impact on a movie streaming platform. 

### Existing studies
In this paper, we will be looking into the Netflix Price Dataset. In 2006 Netflix decided to start a competition with a grand prize of 1 million US dollars. The goal of the competition was to create a collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films. In order to win you had to at least improve on Netflix's own algorithm by 10%. 
During the competition a lot of literature has emerged about the dataset and the competition. [@bell2007lessons; @takacs2008matrix; @narayanan2006break] 

However, no papers or any other literature can be found on network analysis on this dataset. We like to fill this gap in the literature by analysing the network and network structure that arises from the subset of the data that we will use. 
[@guillory2011simultaneous] developed an active movie recommendation system for Netflix. They found that a recommender system should not constantly ask questions to a user, because those reduces the user's mental image of how the recommendation system learns, prompting some participants to "lose track of what they were teaching". According to @amershi2014power, this was because users are not always eager to act as simple oracles (repeatedly telling the recommendation system whether they like something or not). This is interesting to take into account for our research, because this would mean that a social network within a movie recommendation system can never be fully exposed. 

### Research Questions and hypotheses
In this dataset, we can easily see connections between users and movies, but not between just the users or just the movies. At least not, when we do not include one or the other. That is why we have defined the following research questions:

* RQ1: What are the effects of the genre on the likelihood of having both movies watched by the same user?
* RQ2: How much does liking the same movies influence disliking the same movies, and vice versa? (In a network of frequent reviewers) 

In order to be able to answer these research questions, we have made set up the following hypothesis accordingly:

* Research Question 1:
  + Comparable movies, in terms of either genre and/or year, are more often watched by the same user than uncomparable movies.  
  + Movies of genre horror are generally rated higher than other movie genres.
* Research Question 2:
  + Users that like the same movies to a certain degree, are likely to dislike the same movies to that same degree 
  + Users that dislike the same movies to a certain degree, are likely to like the same movies to that same degree 


## Methodology

### Dataset 
The primary dataset that is being used during this study is provided by Netflix  (REF: KAGGLE) . 
Netflix shared this data for the Netflix Prize competition where people could use the data to improve their recomender system for movies.
The data includes ratings for 17770 movies from 480189 users. 
In total, it contains around 24 million ratings on a scale from one to five and the date of the rating.
For the movies, the dataset only contains the title of the movie and the year it was released.
This was too little information about the movies for our project. 
Therefore, we decided to look for an additional data source to enrich the dataset. 
We used Amazon Prime movie genres to add genres to the dataset for further analysis.

The large size of the dataset made it difficult to work with and made running analysis models infeasable. 
Therefore, we decided to only use a selection of the ratings. The dataset has ratings from October 1998 to December 2005. 
For this analysis, only the data from December 2001, Januari 2002, and februari 2002 are considered. 
We chose this winter because this is the first time where users started to rate a good amount of movies while the number of ratings still did not completely explode yet.




#### Descriptives
```{r, echo=FALSE, warning=FALSE}
data <- read.csv("reviews_winter_2001_2002.csv")
data_genres <- read_excel("movie_movie_connections_node_list_genre.xlsx")

#sum_data <- summary(data)
#kable(sum_data)%>% kable_styling(position = "center")
```

Looking at the average Rating of the users in the data, the average rating they gave was about a $3,36$.
```{r, echo=FALSE, warning=FALSE}
sum <- do.call(data.frame, 
               list(mean = apply(data[3], 2, mean),
                    sd = apply(data[3], 2, sd),
                    min = apply(data[3], 2, min),
                    max = apply(data[3], 2, max),
                    n = apply(data[3], 2, length)))
    
kable(sum)%>% kable_styling(position = "center")
```

```{r, echo=FALSE, warning=FALSE}
data$Date_conv <- as.Date(data$Date)
data$day_of_week <- wday(data$Date_conv, label = TRUE, abbr = FALSE)
```

```{r, echo=FALSE, warning=FALSE}
per_day <- table(data$Date_conv)
```


Looking at the histogram of the amound of ratings given on a certain date we can see that a peak in ratings exists always in the middle of the week (around wednesday/thursday). Looking at the distribution of the ratings over time we don't see great differences. Over time, people give consistent ratings.

```{r, fig.height=8, fig.width=12, echo=FALSE, warning=FALSE}
p1 <- ggplot(data, aes(x = Date_conv, fill=day_of_week)) +
    geom_histogram(binwidth=0.5) + 
    scale_x_date(breaks=date_breaks(width="14 day")) + 
    xlab("Date") + ylab("Count ratings") +
    labs(title="Histogram of the amount of ratings given over time", fill="Day") +
    scale_fill_discrete(labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")) +
    theme_minimal()

p2 <- ggplot(data, aes(x = Date_conv, group = as.factor(Rating), fill = as.factor(Rating))) + 
    stat_bin(binwidth=0.5, position = "fill") +
    scale_x_date(breaks=date_breaks(width="14 day")) + 
    xlab("Date") + ylab("Ratings (proportion)") +
    labs(title="Distribution of ratings per day over time", fill="Rating") +
    theme_minimal()

# Here we use a self made function that is in a separate Rscript file.
same.size.ggplot(c("p1", "p2"), "p2")

grid.arrange(p1, p2, ncol=1)
```

The genres in the data were determined using Amazone Prime. The first genre that Amazone Prime displayed is taken as the genre of the movie. As the genres that were displayed in Amazone Prime were not on alfabethic order, we assume that the first genre given is the main genre. 20 genres were found in the Netflix dataset. Most of the movies had *comedy* or *drama* as genre.

```{r, fig.height=4, fig.width=12}
ggplot(data_genres, aes(genre)) +
    geom_bar() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.3), 
          panel.background = element_rect(fill = "white")
          )
```

### Data analysis (Research Rationale)
(about 500 words) – 1 POINTS
*	Why are these two methods suitable for your data?

*	Why are these two methods suitable for your research questions?

*	Are there other methods to address these questions? If yes, why are the 
methods you chose better for this case?

#### QAP
QAP generates graphs based on the permutation of an actually observed graph. This maintains some of the 
structure of the original graph and stays closer to the "nature" of the observed network. 
It is not meant to generate networks, but to generate the empirical distribution of a statistic, under a null hypothesis. FROM slides 

#### ERGM
Many metrics like density and centrality are used to describe the structural characteristics of an observed network. These metrics, on the other hand, describe the observed network, which is just one of many possible alternative networks. The structural characteristics of this group of alternative networks may or may not be similar. A statistical model must consider the set of all possible alternative networks weighted in their similarity to an observed network to credit a statistical inference about the processes that influence the formation of network structures. 

Network data, on the other hand, violates the independence and identical distribution assumptions of statistical models like linear regression because they are inherently relational. Alternative statistical models must reflect the uncertainty associated with a given observation, allow inference about the relative frequency of theoretically important network substructures, eliminate ambiguities in the influence of confounding processes, efficiently represent complex structures, and link local-level processes with global-level properties. For example, degree-preserving randomization is a method of considering an observed network in terms of multiple alternative networks. 

Exponential Random Graph models (ERGM) is a powerful technique for conducting statistical inference on network data. ERGM is a good choice as a network modelling framework for cases where the outcome of interest is in the presence or absence of edges. That makes it a suitable method for our research question, because .... AANVULLEN

## Results

### QAP Model
(about 1000 words) – 2.5 POINTS

#### Constructing data and the model
During preprocessing of the data, an interesting artifact was encountered; When users were removed from the dataset that either liked or disliked a movie, it was clearly shown that it occurred much more often that user gave a 4/5 star rating to a movie than a 1/2 star rating. This is interesting to keep in mind, as this might pose a potential bias in the dataset.

To give a weight to the edges in terms of importance, we calculate the weight according to a formula: 

$$weight = \frac{no. of mutual liked movies}{max(no. of liked movies)} $$


#### Results

##### More than 120 reviews
In order to get a feeling of the QAP regression model, and to get initial results in a reasonable amount of time, we first did the analysis on users who had 120 reviews (like and dislike) or more. The results are shown in the figure below. 
```{r echo = FALSE}
knitr::include_graphics("results120plus.png")
```

As can be seen from the figure, .....

##### More than 100 reviews
the analysis on users who had 100 reviews (like and dislike) or more. The results are shown in the figure below. 
```{r echo = FALSE}
knitr::include_graphics("results100plus.png")
```

As can be seen from the figure, .....

##### More than 80 reviews
the analysis on users who had 80 reviews (like and dislike) or more. The results are shown in the figure below. 
```{r echo = FALSE}
knitr::include_graphics("results80plus.png")
```

As can be seen from the figure, .....

##### More than 60 reviews
the analysis on users who had 60 reviews (like and dislike) or more. The results are shown in the figure below. 
```{r echo = FALSE}
knitr::include_graphics("results60plus.png")
```

As can be seen from the figure, .....

#### Findings in relation to hypothesis


*	Discuss the meaning of your findings in relation to your hypothesis. (half of
the points evaluated in this other part)


```{r echo = FALSE, results = 'asis'}
# table example
cn <- c("age", "gender", "eyes_col")
one <- c(7, "M", "BLUE")
two <- c(8, "F", "BROWN")
three <- c(8, "M", "GREEN")
four <- c(7, "F", "PINK")

tab <- rbind(cn, one, two, three, four)
rownames(tab) <- NULL
knitr::kable(tab)
```


### ERGM 
(about 1000) – 2.5 POINTS


* Present your results appropriately (plots, tables…) and discuss your findings 
in plain English

* Discuss the meaning of your findings in relation to your hypothesis. (half of 
the points evaluated in this other part)

Option 1: 

```{r model 1, echo = FALSE}
# model results display example
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D90 <- lm(weight ~ group - 1)

# install.packages("texreg")

knitr::kable(texreg::matrixreg(lm.D9))


```

Option 2

```{r model 2, echo = FALSE, results = 'asis'}
# model results display example
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D90 <- lm(weight ~ group - 1)

# install.packages("texreg")

knitr::kable(texreg::matrixreg(list(lm.D9, lm.D90)))


```


Option 3

```{r model 3, echo = FALSE}
# model results display example
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D90 <- lm(weight ~ group - 1)

# install.packages("texreg")


texreg::plotreg(lm.D9)


```


Option 4

```{r model 4, echo = FALSE}
# model results display example
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D90 <- lm(weight ~ group - 1)

# install.packages("texreg")

texreg::plotreg(list(lm.D9, lm.D90))

```


## Conclusion

(about 350 words) – 0.7 POINTS
What were your topic and research questions again? (1 sentence)

What did you learn from the two analysis you run? *** most important point to 
address 0.5 POINTS here

Who benefits from your findings?

What does remain an open problem?

### Discussion
What remains an open problem, is that we assume that a frequent reviewer gives a rating to a movie after watching it. However, we don’t have information on (re-)viewers that watch a movie but don’t give a rating afterwards. This is difficult to overcome as we simply don’t have data available on this matter.  

***Can you give suggestions for future work in this area?***
For future research it could be beneficial to dive more into the literature and find out more about what is already known in order to give a stronger reasoning behind the explanatory variables used in the analysis. Also, it would be interesting to look into more recent data as streaming services are widely used nowadays and the variation of users is maybe larger now.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
