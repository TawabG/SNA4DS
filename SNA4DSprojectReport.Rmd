---
title: 'Netflix case study: Genre vs decade'
author:
- name: Alan, Chantal, Dion, Dylan, Suzanne, Tawab
  role: SNA4DS Group 14 students
shorttitle: Research Project Report
output:
  pdf_document: default
  html_document:
    df_print: paged
authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
  Enter author note here.
keywords: keywords
wordcount: X
bibliography: r-references.bib
toc: yes
toc_float: yes
floatsintext: yes
figurelist: no
tablelist: no
footnotelist: no
linenumbers: no
mask: no
draft: no
documentclass: apa6
classoption: man
affiliation:
- id: '1'
  institution: Eindhoven University of Technology
- id: '2'
  institution: Tilburg University
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
source("same_size_ggplot.R")
library(ggplot2)
library(dplyr)
library(kableExtra)
library(scales)
library(gridExtra)
library(lubridate)
library(nycflights13)
library(tidyquant)
library(readxl)
```

```{r analysis-preferences, include = FALSE}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```



# Citations in papaja, detele appropriately later

Add the bibtex entry in the .bib file. You can find the entries in Google scholar, 
but double check since it is not always correct. 

Call the citations in the text:

Citation within parentheses	[@R-papaja]

Multiple citations	[@R-papaja; @R-base]

In-text citations	@R-papaja

Year only	[-@R-base]

Only if your citation appears in the text it will also show up in the Reference 
list. Don't manually modify the Reference list.

## Executive Summary 

(150 words) – 0.3 POINTS
Summarize the report. Write this as the very last thing.

What is the main topic you are addressing?

what are your research questions and hypotheses?  

what are your results and the main conclusion?



## Introduction 
### Main topic
In this paper, we are going to study the presence of social networks within a movie streaming platform. We're focusing on the structure of links among a group of social players, which consist of users watching and rating movies on Netflix.  
Users of Netflix's movie recommendation algorithms are frequently given specific questions about their interests for certain items (which they provide by liking or disliking them, for example). These choices are then immediately integrated into the underlying learning system for future suggestions. If a recommender system starts promoting unwanted products after incorporating new preferences, the user may try to steer the system in the future by correcting it or supplying alternate preference information. 

### Importance
It is important to study the presence of these socials networks because this could potentially improve the recommender engine that this currently in place. For example, if you know that a user is likely to like a movie that other users with the same “liking profile” also like, you can recommend that movie to the user. When these connections are studied thoroughly, you could have a high probability that the recommendation is successful. This could have a large impact on a movie streaming platform. 

### Existing studies
In this paper, we will be looking into the Netflix Price Dataset. In 2006 Netflix decided to start a competition with a grand prize of 1 million US dollars. The goal of the competition was to create a collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films. In order to win you had to at least improve on Netflix's own algorithm by 10%. 
During the competition a lot of literature has emerged about the dataset and the competition. [@bell2007lessons; @takacs2008matrix; @narayanan2006break] 

However, no papers or any other literature can be found on network analysis on this dataset. We like to fill this gap in the literature by analysing the network and network structure that arises from the subset of the data that we will use. 
[@guillory2011simultaneous] developed an active movie recommendation system for Netflix. They found that a recommender system should not constantly ask questions to a user, because those reduces the user's mental image of how the recommendation system learns, prompting some participants to "lose track of what they were teaching". According to @amershi2014power, this was because users are not always eager to act as simple oracles (repeatedly telling the recommendation system whether they like something or not). This is interesting to take into account for our research, because this would mean that a social network within a movie recommendation system can never be fully exposed. 

### Research Questions and hypotheses
In this dataset, we can easily see connections between users and movies, but not between just the users or just the movies. At least not, when we do not include one or the other. That is why we have defined the following research questions:

* RQ1: What are the effects of the genre on the likelihood of having both movies watched by the same user?
* RQ2: How much does liking the same movies influence disliking the same movies, and vice versa? (In a network of frequent reviewers) 

In order to be able to answer these research questions, we have made set up the following hypothesis accordingly:

* Research Question 1:
  + Comparable movies, in terms of either genre and/or year, are more often watched by the same user than uncomparable movies.  
  + Movies of genre horror are generally rated higher than other movie genres.
* Research Question 2:
  + Users that like the same movies to a certain degree, are likely to dislike the same movies to that same degree 
  + Users that dislike the same movies to a certain degree, are likely to like the same movies to that same degree 


## Methodology

### Dataset 
The primary dataset that is being used during this study is provided by Netflix  (REF: KAGGLE) . 
Netflix shared this data for the Netflix Prize competition where people could use the data to improve their recomender system for movies.
The data includes ratings for 17770 movies from 480189 users. 
In total, it contains around 24 million ratings on a scale from one to five and the date of the rating.
For the movies, the dataset only contains the title of the movie and the year it was released.
This was too little information about the movies for our project. 
Therefore, we decided to look for an additional data source to enrich the dataset. 
We used Amazon Prime movie genres to add genres to the dataset for further analysis.

The large size of the dataset made it difficult to work with and made running analysis models infeasable. 
Therefore, we decided to only use a selection of the ratings. The dataset has ratings from October 1998 to December 2005. 
For this analysis, only the data from December 2001, Januari 2002, and februari 2002 are considered. 
We chose this winter because this is the first time where users started to rate a good amount of movies while the number of ratings still did not completely explode yet.

To answer our two research questions we had to transform our data further in two different ways. 
For the first research question, the network is defined as movies for the nodes and the movies have an edge between them if there is an user that likes both movies.
Liking a movie is defined as a user giving a 4 or 5 rating for a movie. 
Of course, modelling the network this way means that movies that are highly rated are always connect.
Therefore, the network only included movies that have between 20 and 50 ratings. 
That means that our analysis only concerns niche movies with a small number of ratings.
Moreover, there is a lower threshold too to make sure that we only include movies that have at least a few connections.
All in all, the network is just under 500 nodes and the attributes for the nodes are the year of release of the movie and the main movie genre that comes from Amazon Prime.





#### Descriptives
```{r, echo=FALSE, warning=FALSE}
data <- read.csv("reviews_winter_2001_2002.csv")
data_genres <- read_excel("movie_movie_connections_node_list_genre.xlsx")

#sum_data <- summary(data)
#kable(sum_data)%>% kable_styling(position = "center")
```

Looking at the average Rating of the users in the data, the average rating they gave was about a $3,36$.
```{r, echo=FALSE, warning=FALSE}
sum <- do.call(data.frame, 
               list(mean = apply(data[3], 2, mean),
                    sd = apply(data[3], 2, sd),
                    min = apply(data[3], 2, min),
                    max = apply(data[3], 2, max),
                    n = apply(data[3], 2, length)))
    
kable(sum)%>% kable_styling(position = "center")
```

```{r, echo=FALSE, warning=FALSE}
data$Date_conv <- as.Date(data$Date)
data$day_of_week <- wday(data$Date_conv, label = TRUE, abbr = FALSE)
```

```{r, echo=FALSE, warning=FALSE}
per_day <- table(data$Date_conv)
```


Looking at the histogram of the amound of ratings given on a certain date we can see that a peak in ratings exists always in the middle of the week (around wednesday/thursday). Looking at the distribution of the ratings over time we don't see great differences. Over time, people give consistent ratings.

```{r, fig.height=8, fig.width=12, echo=FALSE, warning=FALSE}
p1 <- ggplot(data, aes(x = Date_conv, fill=day_of_week)) +
    geom_histogram(binwidth=0.5) + 
    scale_x_date(breaks=date_breaks(width="14 day")) + 
    xlab("Date") + ylab("Count ratings") +
    labs(title="Histogram of the amount of ratings given over time", fill="Day") +
    scale_fill_discrete(labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")) +
    theme_minimal()

p2 <- ggplot(data, aes(x = Date_conv, group = as.factor(Rating), fill = as.factor(Rating))) + 
    stat_bin(binwidth=0.5, position = "fill") +
    scale_x_date(breaks=date_breaks(width="14 day")) + 
    xlab("Date") + ylab("Ratings (proportion)") +
    labs(title="Distribution of ratings per day over time", fill="Rating") +
    theme_minimal()

# Here we use a self made function that is in a separate Rscript file.
same.size.ggplot(c("p1", "p2"), "p2")

grid.arrange(p1, p2, ncol=1)
```

The genres in the data were determined using Amazone Prime. The first genre that Amazone Prime displayed is taken as the genre of the movie. As the genres that were displayed in Amazone Prime were not on alfabethic order, we assume that the first genre given is the main genre. 20 genres were found in the Netflix dataset. Most of the movies had *comedy* or *drama* as genre.

```{r, fig.height=4, fig.width=12}
ggplot(data_genres, aes(genre)) +
    geom_bar() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.3), 
          panel.background = element_rect(fill = "white")
          )
```

### Data analysis (Research Rationale)
(about 500 words) – 1 POINTS
*	Why are these two methods suitable for your data?

*	Why are these two methods suitable for your research questions?

*	Are there other methods to address these questions? If yes, why are the 
methods you chose better for this case?

#### QAP
In general, QAP is very interesting, when you have two or more networks and wants to compare those. In addition, we also choose QAP regression, since we thought it would be interesting to use this model, based on its permutation of actually observed graphs and therefore staying closer to the nature of our networks. 

There are three different QAP models from which one had to be chosen for our data and research question. The first is the simple QAP test where you measure the association between two networks. Furthermore, there is a QAP linear model, which can be used on a valued dependent network and one or more explanatory networks. Finally, there is the QAP logistic model, which differs from the linear model, in terms of a binary-valued dependent network. 

Our weights in our network are not binary so the logistic model is not an option for us. Even though, we only have two networks we decided to go for the linear model since we would like to see if we could “explain” the first network with the second and vice versa.   

#### ERGM
Many metrics like density and centrality are used to describe the structural characteristics of an observed network. These metrics, on the other hand, describe the observed network, which is just one of many possible alternative networks. The structural characteristics of this group of alternative networks may or may not be similar. A statistical model must consider the set of all possible alternative networks weighted in their similarity to an observed network to credit a statistical inference about the processes that influence the formation of network structures. 

Network data, on the other hand, violates the independence and identical distribution assumptions of statistical models like linear regression because they are inherently relational. Alternative statistical models must reflect the uncertainty associated with a given observation and allow inference about the relative frequency of theoretically important network substructures. Exponential Random Graph Models (ERGM) help with this. 

Using ERGMs is a powerful technique for conducting statistical inference on network data. ERGMs are a good choice as a network modelling framework for cases where the outcome of interest is in the presence or absence of edges. 

## Results

### QAP Model
(about 1000 words) – 2.5 POINTS

#### Constructing data and the model
During preprocessing of the data, an interesting artifact was encountered; When users were removed from the dataset that either liked or disliked a movie, it was clearly shown that it occurred much more often that user gave a 4/5 star rating to a movie than a 1/2 star rating. This is interesting to keep in mind, as this might pose a potential bias in the dataset. 

To give a weight to the edges in terms of importance, we calculate the weight according to a formula:  

$$weight = max((\frac{no. of mutual liked movies}{
no. of liked movies})*100) $$


#### Results
We have generated results for different (filtered) versions of the dataset. The results are explained and shown below.

##### More than 120 reviews
In order to get a feeling of the QAP regression model, and to get initial results in a reasonable amount of time, we first did the analysis on users who had 120 reviews in total(like and dislike) or more. The results are shown in the figure below. 
```{r echo = FALSE}
knitr::include_graphics("results120plus.png")
```

As can be seen from the figure, the QAP regression model explains only 6% of the variation in the like/dislike interaction network for movies when looking at liked movie ratings. When looking at disliked movie ratings, it becomes even less: 4%. 

##### More than 100 reviews
Next, we decreased the filter threshold for the number of ratings that users have given in the dataset. This increased the amount of datapoints in the dataset. So, we had a dataset with users who had 100 reviews in total (like and dislike) or more, and ran the analysis. The results are shown in the figure below.  

```{r echo = FALSE}
knitr::include_graphics("results100plus.png")
```

As can be seen from the figure, the QAP regression model explains only 9% of the variation in the like/dislike interaction network for movies when looking at liked movie ratings. When looking at disliked movie ratings, it becomes even less: 6%. 

##### More than 80 reviews
Because of the improved results in the last section, we decreased the filter threshold for the number of ratings that users have given in the dataset even more. This again increased the amount of datapoints in the dataset, and we did the analysis on users who had 80 reviews in total (like and dislike) or more. The results are shown in the figure below. 

```{r echo = FALSE}
knitr::include_graphics("results80plus.png")
```

As can be seen from the figure, the QAP regression model explains 14% of the variation in the like/dislike interaction network for movies when looking at liked movie ratings. When looking at disliked movie ratings, it again becomes a bit less: 10,4%. 

##### More than 60 reviews
For the last time, we decreased the filter threshold for the number of ratings that users have given in the dataset. This also increased the amount of datapoints in the dataset, and we ran the analysis on users who had 80 reviews in total (like and dislike) or more. The results are shown in the figure below. 

```{r echo = FALSE}
knitr::include_graphics("results60plus.png")
```

As can be seen from the figure, the QAP regression model explains 17% of the variation in the like/dislike interaction network for movies when looking at liked movie ratings. When looking at disliked movie ratings, the model can explain 13% of the variation. Th finally gave us the feeling that we are building somewhat meaningful models. 

We even wanted to go a step further and decrease the filter threshold for the number of ratings that users have given to 40. But this would yield a dataset that was too large to analyse, as it costs 25+ hours to run an analysis on it. 
 

#### Findings in relation to hypothesis

When relating our findings to our aforementioned hypothesis, we see that it is possible to build models that somewhat can explain the variance in the network in a significant way. However, we can clearly see that more data is not always giving better explanations. 

Also, it very interesting to see that liking movies says more about which movies you might dislike, than vice versa. An explanation for this might be that we saw in the data that there were more positive ratings than negative ratings, which could explain that disliking defines a more specific taste that is better detectable.  


### ERGM 
(about 1000) – 2.5 POINTS


* Present your results appropriately (plots, tables…) and discuss your findings 
in plain English

* Discuss the meaning of your findings in relation to your hypothesis. (half of 
the points evaluated in this other part)

Option 1: 

```{r model 1, echo = FALSE}
# model results display example
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D90 <- lm(weight ~ group - 1)

# install.packages("texreg")

knitr::kable(texreg::matrixreg(lm.D9))


```

Option 2

```{r model 2, echo = FALSE, results = 'asis'}
# model results display example
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D90 <- lm(weight ~ group - 1)

# install.packages("texreg")

knitr::kable(texreg::matrixreg(list(lm.D9, lm.D90)))


```


Option 3

```{r model 3, echo = FALSE}
# model results display example
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D90 <- lm(weight ~ group - 1)

# install.packages("texreg")


texreg::plotreg(lm.D9)


```


Option 4

```{r model 4, echo = FALSE}
# model results display example
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D90 <- lm(weight ~ group - 1)

# install.packages("texreg")

texreg::plotreg(list(lm.D9, lm.D90))

```


## Conclusion

(about 350 words) – 0.7 POINTS
What were your topic and research questions again? (1 sentence)

What did you learn from the two analysis you run? *** most important point to 
address 0.5 POINTS here

Who benefits from your findings?

What does remain an open problem?

### Discussion
What remains an open problem, is that we assume that a frequent reviewer gives a rating to a movie after watching it. However, we don’t have information on (re-)viewers that watch a movie but don’t give a rating afterwards. This is difficult to overcome as we simply don’t have data available on this matter.  

***Can you give suggestions for future work in this area?***
For future research it could be beneficial to dive more into the literature and find out more about what is already known in order to give a stronger reasoning behind the explanatory variables used in the analysis. Also, it would be interesting to look into more recent data as streaming services are widely used nowadays and the variation of users is maybe larger now.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
